{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TinyImageNet_PerceiverIO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvQ8KOjLdS1W",
        "outputId": "5b6598ea-8fc2-43a7-a4d7-c17d5ad02eca"
      },
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yaxrfrndh9J",
        "outputId": "968810a0-9512-4e82-c620-206b49c4fd9d"
      },
      "source": [
        "# Obtain Dataset\n",
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -qq 'tiny-imagenet-200.zip'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-09 01:21:11--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  52.1MB/s    in 5.0s    \n",
            "\n",
            "2021-10-09 01:21:16 (47.6 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDyt271-dvMk"
      },
      "source": [
        "# Loading Data Set and Assembling Training, Validation, and Test Sets\n",
        "\n",
        "* Taken from Previous Coding Assignment - week-4-opt-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czvJRFARdrw-",
        "outputId": "3d5f5785-6fb2-4bb1-f844-ca791bd5d544"
      },
      "source": [
        "# Load training dataset, then split into training, validation, and test sets\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Number of batches in Training, Validation, Test Sets\n",
        "TRAINING_BATCHES = 100\n",
        "VALIDATION_BATCHES = 50\n",
        "TEST_BATCHES = 50\n",
        "\n",
        "# Load training data\n",
        "train_dir = \"./tiny-imagenet-200/train\"\n",
        "train_data = torchvision.datasets.ImageFolder(train_dir, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle = True)\n",
        "\n",
        "\n",
        "# Loop through data loader and add batches and labels based on constants\n",
        "training_set = []\n",
        "validation_set = []\n",
        "test_set = []\n",
        "i = 0\n",
        "for set_data in train_data_loader:\n",
        "  i += 1\n",
        "  if i <= TRAINING_BATCHES:\n",
        "    training_set.append(set_data)\n",
        "  \n",
        "  elif i <= TRAINING_BATCHES + VALIDATION_BATCHES:\n",
        "    validation_set.append(set_data)\n",
        "  \n",
        "  elif i <= TRAINING_BATCHES + VALIDATION_BATCHES + TEST_BATCHES:\n",
        "    test_set.append(set_data)\n",
        "  \n",
        "  else:\n",
        "    break\n",
        "\n",
        "print(len(training_set))\n",
        "print(len(validation_set))\n",
        "print(len(test_set))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "50\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3noKG4aekl9"
      },
      "source": [
        "!pip install perceiver-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAZ0PGvWe1Mk",
        "outputId": "46653ead-7951-4d5c-bda1-c855ce187408"
      },
      "source": [
        "import torch\n",
        "from perceiver_pytorch import PerceiverIO\n",
        "\n",
        "original_model = PerceiverIO(\n",
        "    dim = 64,                    # dimension of sequence to be encoded\n",
        "    queries_dim = 1,            # dimension of decoder queries\n",
        "    logits_dim = 200,            # dimension of final logits\n",
        "    depth = 6,                   # depth of net\n",
        "    num_latents = 256,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "    latent_dim = 512,            # latent dimension\n",
        "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
        "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
        "    weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        ")\n",
        "\n",
        "seq = torch.randn(3, 64, 64)\n",
        "queries = torch.rand(3, 64, 1)\n",
        "\n",
        "logits = original_model(seq, queries = queries) # (1, 128, 100) - (batch, decoder seq, logits dim)\n",
        "print(logits.squeeze().size())\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAKr7zApfFDm"
      },
      "source": [
        "LR = 1\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-6\n",
        "EPOCHS = 8\n",
        "\n",
        "def train_model(model_in, learning_rate = LR, weight_d = WEIGHT_DECAY, momentum = MOMENTUM):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = model_in\n",
        "  model.to(device)\n",
        "  \n",
        "  # Define Loss Function and get optimizer\n",
        "  train_loss = {}\n",
        "  validation_acc = {}\n",
        "  loss_f = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = LR, weight_decay=WEIGHT_DECAY, betas=(momentum, 0.999))\n",
        "  epoch_loss = {}\n",
        "  for epoch in range (EPOCHS):\n",
        "    print(\"EPOCH : \", epoch)\n",
        "    model.train()\n",
        "    batch_loss = []\n",
        "\n",
        "    # Iterate through training set, collect loss values and update model\n",
        "    for im, truth_labels in training_set:\n",
        "      im = im.mean(1)\n",
        "      # print(im.size())\n",
        "      im = im.to(device)\n",
        "      queries = torch.randn(BATCH_SIZE, 64, 1)\n",
        "      # queries[:, 0, 0] = truth_labels[:]\n",
        "      # print(queries, truth_labels.size())\n",
        "      queries = queries.to(device)\n",
        "      truth_labels = truth_labels.to(device)\n",
        "      predicted_labels = model(im, queries = queries).mean(1).squeeze()\n",
        "      predicted_labels = predicted_labels.to(device)\n",
        "      print(predicted_labels.size())\n",
        "      loss = loss_f(predicted_labels, truth_labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.append(loss.item())\n",
        "      accuracy = (predicted_labels.argmax(1) == truth_labels).float().mean().item()\n",
        "      print(\"******Train ACCURACY****** : \", accuracy)\n",
        "\n",
        "    \n",
        "    print(\"TRAIN_LOSS of Each Batch: \", torch.FloatTensor(batch_loss).mean().item())\n",
        "    epoch_loss[epoch] = batch_loss\n",
        "\n",
        "    # Iterate through validation set and compute validation accuracy\n",
        "    model.eval()\n",
        "    accuracies = []\n",
        "    for validation_im, validation_labels in validation_set:\n",
        "      validation_im = validation_im.to(device)\n",
        "      validation_im = validation_im.mean(1)\n",
        "      validation_labels = validation_labels.to(device)\n",
        "      queries = torch.randn(BATCH_SIZE, 1, 1)\n",
        "      queries[:, 0, 0] = validation_labels[:]\n",
        "      queries = queries.to(device)\n",
        "      predicted_labels = model(validation_im, queries = queries).squeeze().argmax(1)\n",
        "      accuracy = (predicted_labels == validation_labels).float().mean().item()\n",
        "      accuracies.append(accuracy)\n",
        "    \n",
        "    validation_set_accuracy = torch.FloatTensor(accuracies).mean().item()\n",
        "    print(\"******VALIDATION ACCURACY****** : \", validation_set_accuracy)\n",
        "    validation_acc[epoch] = validation_set_accuracy\n",
        "\n",
        "  # # Calculate test set accuracy\n",
        "  # model.eval()\n",
        "  # accuracies = []\n",
        "  # for test_im, test_labels in test_set:\n",
        "  #     test_im = test_im.to(device)\n",
        "  #     test_labels = test_labels.to(device)\n",
        "\n",
        "  #     predicted_labels = model(test_im).argmax(1)\n",
        "  #     accuracy = (predicted_labels == test_labels).float().mean().item()\n",
        "  #     accuracies.append(accuracy)\n",
        "    \n",
        "  # test_set_accuracy = torch.FloatTensor(accuracies).mean().item()\n",
        "  # print(\"******TEST ACCURACY****** : \", test_set_accuracy)\n",
        "  # return train_loss, validation_acc, test_set_accuracy"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zrsEVLK3g7Kq",
        "outputId": "fbc254c2-38a1-4b9c-9536-4d92d56d42d5"
      },
      "source": [
        "train_model(original_model)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH :  0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n",
            "******Train ACCURACY****** :  0.0\n",
            "torch.Size([8, 200])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-4d29804edb82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-93-564a9bfa9d98>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_in, learning_rate, weight_d, momentum)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}