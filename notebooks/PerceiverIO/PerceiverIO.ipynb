{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PerceiverIO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvQ8KOjLdS1W",
        "outputId": "5588af06-3b26-4734-8652-9c82e6a64628"
      },
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDyt271-dvMk"
      },
      "source": [
        "# Loading Data Set and Assembling Training, Validation, and Test Sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S0k8XEaUjb_",
        "outputId": "fd4f51fb-8007-49a2-d633-6ad186c67f63"
      },
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -qq 'tiny-imagenet-200.zip'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-09 22:46:31--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  18.2MB/s    in 14s     \n",
            "\n",
            "2021-10-09 22:46:46 (16.7 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czvJRFARdrw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e01890e-ffa7-461f-e030-eae8aad7e62f"
      },
      "source": [
        "# Load training dataset, then split into training, validation, and test sets\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchvision import datasets,transforms\n",
        "import torch.utils.tensorboard as tb\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "dataset = \"CIFAR-10\"\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# create datasets\n",
        "if dataset == \"STL-10\":\n",
        "    training_dataset = datasets.STL10(root='./data/STL10', split='train', transform=transforms.ToTensor(), download=True)\n",
        "    validation_dataset = datasets.STL10(root='./data/STL10', split='test', transform=transforms.ToTensor(), download=True)\n",
        "    in_size = 96\n",
        "    num_classes = 10\n",
        "if dataset == \"CIFAR-10\":\n",
        "    training_dataset = datasets.CIFAR10(root='./data/CIFAR10', train=True, download=True, transform=transforms.ToTensor())\n",
        "    validation_dataset = datasets.CIFAR10(root='./data/CIFAR10', train=False, download=True, transform=transforms.ToTensor())\n",
        "    in_size = 32\n",
        "    num_classes = 10\n",
        "if dataset == \"TinyImageNet\":\n",
        "    training_dataset = datasets.ImageFolder(\"./tiny-imagenet-200/train\", transform=transforms.Compose([transforms.ToTensor()]))\n",
        "    validation_dataset = datasets.ImageFolder(\"./tiny-imagenet-200/val\", transform=transforms.Compose([transforms.ToTensor()]))\n",
        "    in_size = 64\n",
        "    num_classes = 200\n",
        "\n",
        "# create dataloaders\n",
        "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=int(BATCH_SIZE), shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=int(BATCH_SIZE), shuffle=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3noKG4aekl9",
        "outputId": "f460c1ce-3561-4043-d531-3990d303aacf"
      },
      "source": [
        "!pip install perceiver-pytorch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting perceiver-pytorch\n",
            "  Downloading perceiver_pytorch-0.7.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from perceiver-pytorch) (1.9.0+cu111)\n",
            "Collecting einops>=0.3\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->perceiver-pytorch) (3.7.4.3)\n",
            "Installing collected packages: einops, perceiver-pytorch\n",
            "Successfully installed einops-0.3.2 perceiver-pytorch-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAZ0PGvWe1Mk",
        "outputId": "b4f853f8-3bce-4c0e-f166-a06ce742e97c"
      },
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "from perceiver_pytorch import PerceiverIO\n",
        "\n",
        "\n",
        "# class QueryFFN(nn.Module):\n",
        "#     def __init__(self, in_size):\n",
        "#         super().__init__()\n",
        "#         self.query = nn.Linear(in_size*in_size, num_classes)\n",
        "      \n",
        "#     def forward(self, images):\n",
        "#         images = torch.flatten(images, start_dim = 1)\n",
        "#         return self.query(images)[:, None, :]\n",
        "\n",
        "\n",
        "original_model = PerceiverIO(\n",
        "    dim = in_size,                    # dimension of sequence to be encoded\n",
        "    queries_dim = num_classes,            # dimension of decoder queries\n",
        "    logits_dim = num_classes,            # dimension of final logits\n",
        "    depth = 6,                   # depth of net\n",
        "    num_latents = 256,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "    latent_dim = 512,            # latent dimension\n",
        "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
        "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
        "    weight_tie_layers = False,    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "    decoder_ff = False\n",
        ")\n",
        "\n",
        "seq = torch.randn(3, in_size, in_size)\n",
        "queries = torch.rand(3, 1, num_classes)\n",
        "\n",
        "logits = original_model(seq, queries = queries) # (1, 128, 100) - (batch, decoder seq, logits dim)\n",
        "print(logits.squeeze().size())\n",
        "\n",
        "# model = QueryFFN(in_size)\n",
        "\n",
        "# logits = model(seq)\n",
        "# print(logits.size())\n",
        "temp = torch.nn.Parameter(torch.rand(BATCH_SIZE, 1, num_classes), requires_grad = True)\n",
        "original_model.register_parameter(name='query_io', param=temp)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAKr7zApfFDm"
      },
      "source": [
        "LR = 1e-1\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-6\n",
        "EPOCHS = 8\n",
        "\n",
        "def train_model(model_in, learning_rate = LR, weight_d = WEIGHT_DECAY, momentum = MOMENTUM):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = model_in\n",
        "  model = model.to(device)\n",
        "  # query_model = QueryFFN(in_size)\n",
        "  # query_model = query_model.to(device)\n",
        "\n",
        "  # Define Loss Function and get optimizer\n",
        "  train_loss = {}\n",
        "  validation_acc = {}\n",
        "  loss_f = torch.nn.CrossEntropyLoss()\n",
        "  # optimizer = torch.optim.Adam(model.parameters(), lr = LR, weight_decay=WEIGHT_DECAY, betas=(momentum, 0.999))\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY, momentum=MOMENTUM)\n",
        "  # query_optimizer = torch.optim.Adam(query_model.parameters(), lr = 1e-2, weight_decay = WEIGHT_DECAY)\n",
        "  epoch_loss = {}\n",
        "  for epoch in range (EPOCHS):\n",
        "    print(\"EPOCH : \", epoch)\n",
        "    model.train()\n",
        "    # query_model.train()\n",
        "    batch_loss = []\n",
        "    \n",
        "    accuracies = []\n",
        "    num = 0\n",
        "    # Iterate through training set, collect loss values and update model\n",
        "    for im, truth_labels in training_loader:\n",
        "      num += 1\n",
        "      if num > 50:\n",
        "        break\n",
        "\n",
        "      im = im.mean(1)\n",
        "      # print(im.size())\n",
        "      im = im.to(device)\n",
        "      queries = model.query_io.data\n",
        "      queries = queries.to(device)\n",
        "      truth_labels = truth_labels.to(device)\n",
        "      predicted_labels = model(im, queries = queries).squeeze()\n",
        "      predicted_labels = predicted_labels.to(device)\n",
        "      loss = loss_f(predicted_labels, truth_labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      # print(loss)\n",
        "      # query_optimizer.step()\n",
        "      # query_optimizer.zero_grad()\n",
        "      batch_loss.append(loss.item())\n",
        "      accuracy = (predicted_labels.argmax(1) == truth_labels).float().mean().item()\n",
        "      # print(accuracy)\n",
        "      accuracies.append(accuracy)\n",
        "\n",
        "    print(\"******Train ACCURACY****** : \", torch.FloatTensor(accuracies).mean().item())\n",
        "    print(\"TRAIN_LOSS of Each Batch: \", torch.FloatTensor(batch_loss).mean().item())\n",
        "    epoch_loss[epoch] = batch_loss\n",
        "\n",
        "    # # Iterate through validation set and compute validation accuracy\n",
        "    # model.eval()\n",
        "    # query_model.eval()\n",
        "    # accuracies = []\n",
        "    # num = 0\n",
        "    # for validation_im, validation_labels in validation_loader:\n",
        "    #   validation_im = validation_im.to(device)\n",
        "    #   validation_im = validation_im.mean(1)\n",
        "    #   validation_labels = validation_labels.to(device)\n",
        "    #   queries = query_model(validation_im)\n",
        "    #   queries = queries.to(device)\n",
        "    #   predicted_labels = model(validation_im, queries = queries).squeeze().argmax(1)\n",
        "    #   accuracy = (predicted_labels == validation_labels).float().mean().item()\n",
        "    #   accuracies.append(accuracy)\n",
        "    \n",
        "    # validation_set_accuracy = torch.FloatTensor(accuracies).mean().item()\n",
        "    # print(\"******VALIDATION ACCURACY****** : \", validation_set_accuracy)\n",
        "    # validation_acc[epoch] = validation_set_accuracy\n",
        "\n",
        "  # # Calculate test set accuracy\n",
        "  # model.eval()\n",
        "  # accuracies = []\n",
        "  # for test_im, test_labels in test_set:\n",
        "  #     test_im = test_im.to(device)\n",
        "  #     test_labels = test_labels.to(device)\n",
        "\n",
        "  #     predicted_labels = model(test_im).argmax(1)\n",
        "  #     accuracy = (predicted_labels == test_labels).float().mean().item()\n",
        "  #     accuracies.append(accuracy)\n",
        "    \n",
        "  # test_set_accuracy = torch.FloatTensor(accuracies).mean().item()\n",
        "  # print(\"******TEST ACCURACY****** : \", test_set_accuracy)\n",
        "  # return train_loss, validation_acc, test_set_accuracy"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrsEVLK3g7Kq",
        "outputId": "f97cbdef-156b-49fe-ffd9-926f0c62e8e3"
      },
      "source": [
        "train_model(original_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH :  0\n",
            "******Train ACCURACY****** :  0.11124999821186066\n",
            "TRAIN_LOSS of Each Batch:  2.329042911529541\n",
            "EPOCH :  1\n",
            "******Train ACCURACY****** :  0.09125000238418579\n",
            "TRAIN_LOSS of Each Batch:  2.3251492977142334\n",
            "EPOCH :  2\n",
            "******Train ACCURACY****** :  0.08874999731779099\n",
            "TRAIN_LOSS of Each Batch:  2.3311307430267334\n",
            "EPOCH :  3\n",
            "******Train ACCURACY****** :  0.10750000178813934\n",
            "TRAIN_LOSS of Each Batch:  2.316765546798706\n",
            "EPOCH :  4\n",
            "******Train ACCURACY****** :  0.12125000357627869\n",
            "TRAIN_LOSS of Each Batch:  2.313443660736084\n",
            "EPOCH :  5\n",
            "******Train ACCURACY****** :  0.1262499988079071\n",
            "TRAIN_LOSS of Each Batch:  2.3125839233398438\n",
            "EPOCH :  6\n",
            "******Train ACCURACY****** :  0.08375000208616257\n",
            "TRAIN_LOSS of Each Batch:  2.3394484519958496\n",
            "EPOCH :  7\n",
            "******Train ACCURACY****** :  0.14875000715255737\n",
            "TRAIN_LOSS of Each Batch:  2.2910404205322266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KCEjSk5FeLH"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 103,
      "outputs": []
    }
  ]
}